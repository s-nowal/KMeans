#import libraries
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
import cv2
import os
from sklearn.preprocessing import StandardScaler, LabelEncoder

#importing sentiment analysis dataset
data1 = pd.read_csv("/home/sanvi/Downloads/sentiment_analysis_dataset.csv")
x_train = data1.iloc[:,0].values
y_train = data1.iloc[:,1].values

#implementing bayes theorem
vectorizer = CountVectorizer()
x_train = vectorizer.fit_transform(x_train)

classifier = MultinomialNB()
classifier.fit(x_train, y_train)

#importing alphabet data in chunks to save memory space (kernel kept dying) and specifying datatypes
from collections import defaultdict
dtype_dict = defaultdict(lambda: np.float64)
dtype_dict = {'label': 'str','row 1 col 1 pixel': 'str','row 1 col 2 pixel': 'str','row 1 col 3 pixel': 'str','row 1 col 4 pixel': 'str'}

x_chunks = []; y_chunks = []
chunk_size = 10000  

#scaling data and filtering it
scaler = StandardScaler()
label_encoder = LabelEncoder(); label_encoder_fitted = False
data = pd.read_csv('/home/sanvi/Downloads/alphabets_28x28.csv', chunksize=chunk_size, dtype = dtype_dict)

cleaned_chunks = []
for chunk in data:
    cleaned_chunk = chunk.dropna()
    cleaned_chunks.append(cleaned_chunk)

cleaned_df = pd.concat(cleaned_chunks)

for cleaned_chunk in cleaned_chunks:
    x_chunk = cleaned_chunk.iloc[:, 1:].values  # Assuming first column is 'label'
    y_chunk = cleaned_chunk.iloc[:, 0].values
    #if not label_encoder_fitted:
    label_encoder.fit(y_chunk)
    #label_encoder_fitted = True 
    y_chunk_encoded = label_encoder.transform(y_chunk)
    x_chunk_scaled = scaler.fit_transform(x_chunk)
    x_chunks.append(x_chunk_scaled)
    y_chunks.append(y_chunk_encoded)

if x_chunks and y_chunks:
    x = np.concatenate(x_chunks, axis=0)
    y = np.concatenate(y_chunks, axis=0)

#making neural network
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28, 1)),  # Input shape is (28, 28)
    tf.keras.layers.Flatten(),              # Flatten the input 28x28 matrix into a 1D vector
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=26, activation='softmax')  # 26 units for 26 classes (A-Z)
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x,y, epochs = 10)

#preprocessing the image (used ChatGPT for this, was throwing error without it)
def preprocess_image(img_path):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    _, img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    return img

#identifying lines in image
def segment_lines(img):
    lines = img.split('\n')
    return lines

#identifying words in image
def segment_words(line):
    words = line.split(' ')
    return words

#identifying characters in image
def segment_characters(img):
    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])
    char_images = []
    for ctr in contours:
        x, y, w, h = cv2.boundingRect(ctr)
        char_img = img[y:y+h, x:x+w]
        char_images.append(char_img)
    return char_images

#running model on characters
def recognize_characters(char_images):
    recognized_text = ""
    for char_img in char_images:
        char_img = cv2.resize(char_img, (28, 28))
        char_img = np.invert(np.array([char_img]))
        char_img = char_img.reshape((1, 28, 28, 1))
        prediction = model.predict(char_img)
        recognized_char = chr(np.argmax(prediction) + ord('A'))  
        recognized_text += recognized_char
    return recognized_text

#adding all functions
def read_handwritten_multiline(img_path):
    img = preprocess_image(img_path)
    lines = segment_lines(img) 
    recognized_sentences = []
    for line in lines:
        words = segment_words(line)
        recognized_words = []
        for word in words:
            char_images = segment_characters(word)
            recognized_word = recognize_characters(char_images)
            recognized_words.append(recognized_word)
        recognized_sentence = ' '.join(recognized_words)
        recognized_sentences.append(recognized_sentence)    
    return recognized_sentences

#running function on target images and checking accuracy
image_number = 1; predict_i = []
while os.path.isfile(f"/img/line_{image_number}.png"):
    predict_i.append (read_handwritten_multiline(f"/img/line_{image_number}.png"))

predict_final = classifier.predict(predict_i)
actual = pd.read_csv("target_score")
actual_final = actual.iloc[:,1].values

from sklearn.metrics import accuracy_score
accuracy = accuracy_score (actual_final,predict_final)
